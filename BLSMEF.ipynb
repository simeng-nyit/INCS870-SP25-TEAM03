{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKaD_ch-llmE",
        "outputId": "d1cb10e3-fb11-44b0-e3dd-5ef3d83e3b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not running in Colab - proceeding without memory optimization\n",
            "\n",
            "Data Dimensions:\n",
            "X_train: (175341, 193), y_train: (175341, 2)\n",
            "X_test: (82332, 193), y_test: (82332, 2)\n",
            "\n",
            "Running Original BLS:\n"
          ]
        }
      ],
      "source": [
        "# Full code with merge exclusive features\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import numpy as np\n",
        "from scipy import linalg as LA\n",
        "import time\n",
        "from google.colab import runtime\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from BroadLearningSystemMergeExclusiveFeatures import BLS\n",
        "\n",
        "def load_and_preprocess():\n",
        "    # Load data\n",
        "    train = pd.read_csv('UNSW_NB15_training-set.csv')\n",
        "    test = pd.read_csv('UNSW_NB15_testing-set.csv')\n",
        "\n",
        "    # Drop columns\n",
        "    train = train.drop(train.columns[[0, -2]], axis=1).iloc[:, 1:]\n",
        "    test = test.drop(test.columns[[0, -2]], axis=1).iloc[:, 1:]\n",
        "\n",
        "    # Categorical encoding\n",
        "    cat_cols = ['proto', 'service', 'state']\n",
        "    encoder = OneHotEncoder(sparse_output=False,\n",
        "                          handle_unknown='ignore',  # Add this parameter\n",
        "                          categories='auto')  # Explicitly set categories\n",
        "\n",
        "    train_encoded = encoder.fit_transform(train[cat_cols])\n",
        "    # Fixed code using explicit column dropping\n",
        "    train = pd.concat([\n",
        "        train.drop(columns=cat_cols),  # Explicit column specification\n",
        "        pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(cat_cols))\n",
        "    ], axis=1)\n",
        "\n",
        "    # Modified with error handling and validation\n",
        "    try:\n",
        "        test_encoded = encoder.transform(test[cat_cols])\n",
        "    except ValueError as e:\n",
        "        print(f\"Validation error: {str(e)}\")\n",
        "        # Handle unknown categories by combining train+test\n",
        "        full_data = pd.concat([train[cat_cols], test[cat_cols]])\n",
        "        encoder.fit(full_data)\n",
        "        train_encoded = encoder.transform(train[cat_cols])\n",
        "        test_encoded = encoder.transform(test[cat_cols])\n",
        "\n",
        "    test = pd.concat([\n",
        "        test.drop(columns=cat_cols),\n",
        "        pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(cat_cols))\n",
        "    ], axis=1)\n",
        "\n",
        "    # Split features and labels\n",
        "    X_train = train.drop(columns=['label'])\n",
        "    y_train = train['label']\n",
        "    X_test = test.drop(columns=['label'])\n",
        "    y_test = test['label']\n",
        "\n",
        "    # Prescale with RobustScaler\n",
        "    prescaler = RobustScaler(quantile_range=(5, 95))\n",
        "    X_train = prescaler.fit_transform(X_train)\n",
        "    X_test = prescaler.transform(X_test)\n",
        "\n",
        "    # Standard scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "    X_test = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "    # Label encoding\n",
        "    label_encoder = OneHotEncoder(sparse_output=False)\n",
        "    y_train = label_encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "    y_test = label_encoder.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def run_comparison():\n",
        "    X_train, y_train, X_test, y_test = load_and_preprocess()\n",
        "\n",
        "    print(\"\\nData Dimensions:\")\n",
        "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "    print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "    params = {\n",
        "        's': 0.5, 'c': 1e-5, 'N1': 10, 'N2': 10, 'N3': 50\n",
        "    }\n",
        "\n",
        "    print(\"\\nRunning Original BLS:\")\n",
        "    orig = BLS(X_train, y_train, X_test, y_test, K=None, **params)\n",
        "\n",
        "    print(\"\\nRunning BLS with Feature Bundling:\")\n",
        "    bundled = BLS(X_train, y_train, X_test, y_test, K=3, **params)\n",
        "\n",
        "    print(\"\\nComparison Results:\")\n",
        "    print(f\"{'Metric':<20} | {'Original':<10} | {'Bundled':<10}\")\n",
        "    print(\"-\" * 45)\n",
        "    print(f\"{'Training Accuracy':<20} | {orig[2][0]:<10.4f} | {bundled[2][0]:<10.4f}\")\n",
        "    print(f\"{'Testing Accuracy':<20} | {orig[0]:<10.4f} | {bundled[0]:<10.4f}\")\n",
        "    print(f\"{'Training Time (s)':<20} | {orig[3][0]:<10.4f} | {bundled[3][0]:<10.4f}\")\n",
        "    print(f\"{'Testing Time (s)':<20} | {orig[1]:<10.4f} | {bundled[1]:<10.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Colab-specific memory optimization\n",
        "    try:\n",
        "        from google.colab import data_storage\n",
        "        data_storage.enable_garbage_collection(True)\n",
        "        print(\"Colab garbage collection optimized\")\n",
        "    except:\n",
        "        print(\"Not running in Colab - proceeding without memory optimization\")\n",
        "\n",
        "    # GPU memory growth (if using TensorFlow)\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        physical_devices = tf.config.list_physical_devices('GPU')\n",
        "        if physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "            print(\"GPU memory growth enabled\")\n",
        "    except:\n",
        "        pass\n",
        "    run_comparison()"
      ]
    }
  ]
}