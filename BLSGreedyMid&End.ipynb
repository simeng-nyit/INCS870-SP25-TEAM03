{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2C5Gu_L2VKU",
        "outputId": "ff050c6f-a9ce-4647-ea47-141379464de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original dimensions:\n",
            "Training set: (175341, 45), Test set: (82332, 45)\n",
            "\n",
            "After dropping columns:\n",
            "Training set: (175341, 43), Test set: (82332, 43)\n",
            "\n",
            "Final preprocessed dimensions:\n",
            "X_train: (175341, 193), y_train: (175341, 2)\n",
            "X_test: (82332, 193), y_test: (82332, 2)\n",
            "\n",
            "BLS Input Dimensions:\n",
            "Training data: (175341, 193) → (175341, 2)\n",
            "Testing data: (82332, 193) → (82332, 2)\n",
            "============================================================\n",
            "Running Original BLS:\n",
            "Training Accuracy:   93.44%\n",
            "Training Time:       10.51s\n",
            "Testing Accuracy:    45.47%\n",
            "Testing Time:        2.53s\n",
            "\n",
            "============================================================\n",
            "Running BLS with Greedy Bundling:\n",
            "\n",
            "============================================================\n",
            "Feature Bundle Diagnostics:\n",
            "Original input features: 193\n",
            "Feature mapping dimension: 10 groups x 10 nodes = 100\n",
            "Reduced to 12 bundles with max conflict 3\n",
            "\n",
            "Bundle Details (feature mapping columns grouped):\n",
            "Bundle 1: [np.int64(6), np.int64(10), np.int64(18), np.int64(36), np.int64(37), np.int64(45), np.int64(46), np.int64(47), np.int64(50), np.int64(54), np.int64(59), np.int64(70), np.int64(75), np.int64(76), np.int64(80), np.int64(84), np.int64(90), np.int64(97)] (18 features)\n",
            "Bundle 2: [np.int64(11), np.int64(14), np.int64(23), np.int64(30), np.int64(42), np.int64(43), np.int64(49), np.int64(56), np.int64(63), np.int64(65), np.int64(66), np.int64(77), np.int64(79), np.int64(81), np.int64(99)] (15 features)\n",
            "Bundle 3: [np.int64(35), np.int64(44), np.int64(52), np.int64(55), np.int64(60), np.int64(68), np.int64(69), np.int64(83), np.int64(86), np.int64(96)] (10 features)\n",
            "Bundle 4: [np.int64(0), np.int64(9), np.int64(15), np.int64(17), np.int64(20), np.int64(87)] (6 features)\n",
            "Bundle 5: [np.int64(4), np.int64(24), np.int64(29), np.int64(48), np.int64(64), np.int64(72), np.int64(94)] (7 features)\n",
            "Bundle 6: [np.int64(2), np.int64(7), np.int64(32), np.int64(38), np.int64(78), np.int64(92), np.int64(93)] (7 features)\n",
            "Bundle 7: [np.int64(28), np.int64(31), np.int64(58), np.int64(61), np.int64(85), np.int64(88), np.int64(91)] (7 features)\n",
            "Bundle 8: [np.int64(1), np.int64(21), np.int64(33), np.int64(34), np.int64(41), np.int64(51)] (6 features)\n",
            "Bundle 9: [np.int64(12), np.int64(26), np.int64(27), np.int64(57), np.int64(74), np.int64(95)] (6 features)\n",
            "Bundle 10: [np.int64(3), np.int64(5), np.int64(8), np.int64(40), np.int64(67), np.int64(71), np.int64(98)] (7 features)\n",
            "Bundle 11: [np.int64(16), np.int64(19), np.int64(39), np.int64(53), np.int64(62), np.int64(89)] (6 features)\n",
            "Bundle 12: [np.int64(13), np.int64(22), np.int64(25), np.int64(73), np.int64(82)] (5 features)\n",
            "\n",
            "Dimension Comparison:\n",
            "Stage                | Features   | Reduction %\n",
            "Original Input       | 193        | -\n",
            "Feature Mapping      | 100        | -\n",
            "After Bundling       | 12         | 88.0%\n",
            "============================================================\n",
            "\n",
            "Applying to test data:\n",
            "Original test features: 100\n",
            "Using 12 bundles from training\n",
            "Bundled test features: 12\n",
            "\n",
            "Training Accuracy:   90.76%\n",
            "Training Time:       47.27s\n",
            "Testing Accuracy:    55.06%\n",
            "Testing Time:        2.49s\n",
            "\n",
            "============================================================\n",
            "Metric                    | Original BLS    | Bundled BLS    \n",
            "------------------------------------------------------------\n",
            "Training Accuracy         | 0.9344          | 0.9076         \n",
            "Training Time (s)         | 10.5083         | 47.2680        \n",
            "Testing Accuracy          | 0.4547          | 0.5506         \n",
            "Testing Time (s)          | 2.5267          | 2.4937         \n"
          ]
        }
      ],
      "source": [
        "# Full code with greedy bundling\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import numpy as np\n",
        "from scipy import linalg as LA\n",
        "import time\n",
        "from BLSGreedy import BLS\n",
        "\n",
        "# 数据预处理部分\n",
        "def load_and_preprocess():\n",
        "    # 读取数据\n",
        "    train_data = pd.read_csv('UNSW_NB15_training-set.csv')\n",
        "    # train_data = train_data.iloc[:, 1:]\n",
        "\n",
        "    test_data = pd.read_csv('UNSW_NB15_testing-set.csv')\n",
        "    # test_data = test_data.iloc[:, 1:]\n",
        "\n",
        "    print(\"\\nOriginal dimensions:\")\n",
        "    print(f\"Training set: {train_data.shape}, Test set: {test_data.shape}\")\n",
        "\n",
        "    # Safely remove columns by position\n",
        "    def drop_columns(df):\n",
        "        # Column 0 = 'id', Column -2 = 'attack_cat' (before label)\n",
        "        return df.drop(df.columns[[0, -2]], axis=1)\n",
        "\n",
        "    train_data = drop_columns(train_data)\n",
        "    test_data = drop_columns(test_data)\n",
        "\n",
        "    print(\"\\nAfter dropping columns:\")\n",
        "    print(f\"Training set: {train_data.shape}, Test set: {test_data.shape}\")\n",
        "\n",
        "    # Rest of the original preprocessing code remains unchanged\n",
        "    train_data = train_data.iloc[:, 1:]  # Original first column removal (keep if needed)\n",
        "    test_data = test_data.iloc[:, 1:]\n",
        "\n",
        "\n",
        "    # 处理分类特征\n",
        "    categorical_cols = ['proto', 'service', 'state']\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "    # 训练集编码\n",
        "    train_encoded = encoder.fit_transform(train_data[categorical_cols])\n",
        "    train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "    train_data = pd.concat([train_data.drop(categorical_cols, axis=1), train_encoded_df], axis=1)\n",
        "\n",
        "    # 测试集编码\n",
        "    test_encoded = encoder.transform(test_data[categorical_cols])\n",
        "    test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "    test_data = pd.concat([test_data.drop(categorical_cols, axis=1), test_encoded_df], axis=1)\n",
        "\n",
        "    # 分割特征标签\n",
        "    X_train = train_data.drop('label', axis=1)\n",
        "    y_train = train_data['label']\n",
        "    X_test = test_data.drop('label', axis=1)\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    # 标签编码\n",
        "    label_encoder = OneHotEncoder(sparse_output=False)\n",
        "    y_train_onehot = label_encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
        "    y_test_onehot = label_encoder.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "    # 处理剩余分类特征\n",
        "    categorical_cols_x = X_train.select_dtypes(include=['object']).columns\n",
        "    if not categorical_cols_x.empty:\n",
        "        encoder_x = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        X_train_encoded = encoder_x.fit_transform(X_train[categorical_cols_x])\n",
        "        X_test_encoded = encoder_x.transform(X_test[categorical_cols_x])\n",
        "        X_train = pd.concat([X_train.drop(categorical_cols_x, axis=1),\n",
        "                            pd.DataFrame(X_train_encoded, columns=encoder_x.get_feature_names_out(categorical_cols_x))], axis=1)\n",
        "        X_test = pd.concat([X_test.drop(categorical_cols_x, axis=1),\n",
        "                           pd.DataFrame(X_test_encoded, columns=encoder_x.get_feature_names_out(categorical_cols_x))], axis=1)\n",
        "\n",
        "    # 标准化\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Final dimensions before returning\n",
        "    print(\"\\nFinal preprocessed dimensions:\")\n",
        "    print(f\"X_train: {X_train_scaled.shape}, y_train: {y_train_onehot.shape}\")\n",
        "    print(f\"X_test: {X_test_scaled.shape}, y_test: {y_test_onehot.shape}\")\n",
        "\n",
        "    return X_train_scaled, y_train_onehot, X_test_scaled, y_test_onehot\n",
        "\n",
        "\n",
        "def run_comparison():\n",
        "    X_train, y_train, X_test, y_test = load_and_preprocess()\n",
        "\n",
        "    # Print input dimensions for BLS\n",
        "    print(\"\\nBLS Input Dimensions:\")\n",
        "    print(f\"Training data: {X_train.shape} → {y_train.shape}\")\n",
        "    print(f\"Testing data: {X_test.shape} → {y_test.shape}\")\n",
        "\n",
        "    params = {\n",
        "        's': 0.5,      # Shrinkage coefficient\n",
        "        'c': 1e-5,     # Regularization coefficient\n",
        "        'N1': 10,      # Nodes per feature window\n",
        "        'N2': 10,      # Number of feature windows\n",
        "        'N3': 50       # Enhancement nodes\n",
        "    }\n",
        "\n",
        "    print(\"=\"*60 + \"\\nRunning Original BLS:\")\n",
        "    orig_results = BLS(X_train, y_train, X_test, y_test, K=None, **params)\n",
        "    # orig_results = BLS(X_train, y_train, X_test, y_test, **params)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\nRunning BLS with Greedy Bundling:\")\n",
        "    bundled_results = BLS(X_train, y_train, X_test, y_test, K=3, **params)\n",
        "    # bundled_results = BLS(X_train, y_train, X_test, y_test, **params)\n",
        "\n",
        "    # Print comparison table for greedy bundling in the middle\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'Metric':<25} | {'Original BLS':<15} | {'Bundled BLS':<15}\")\n",
        "    print(\"-\"*60)\n",
        "    print(f\"{'Training Accuracy':<25} | {orig_results[2]:<15.4f} | {bundled_results[2]:<15.4f}\")\n",
        "    print(f\"{'Training Time (s)':<25} | {orig_results[3]:<15.4f} | {bundled_results[3]:<15.4f}\")\n",
        "    print(f\"{'Testing Accuracy':<25} | {orig_results[0]:<15.4f} | {bundled_results[0]:<15.4f}\")\n",
        "    print(f\"{'Testing Time (s)':<25} | {orig_results[1]:<15.4f} | {bundled_results[1]:<15.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_comparison()"
      ]
    }
  ]
}